<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>[Under Construction] Sparse Attention Vectors: Generative Multimodal Model Features Are Discriminative Vision-Language Classifiers</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">[Under Construction] Sparse Attention Vectors: Generative Multimodal Model Features Are Discriminative Vision-Language Classifiers</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=k85XvocAAAAJ&hl=en" target="_blank">Chancharik Mitra</a><sup>*</sup>,
                  </span>
                  <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=dyD6nsgAAAAJ&hl=en" target="_blank">Brandon Huang</a><sup>*</sup>,
                  </span>
                  <span class="author-block">
                  <a href="https://www.linkedin.com/in/tianning-chai-64b1312a9/" target="_blank">Tianning Chai</a>,
                  </span>
                  <span class="author-block">
                  <a href="https://linzhiqiu.github.io/" target="_blank">Zhiqiu Lin</a>,
                  </span>
                  <span class="author-block">
                  <a href="https://research.ibm.com/people/assaf-arbelle" target="_blank">Assaf Arbelle</a>,
                  </span>
                  <span class="author-block">
                  <a href="https://www.rogerioferis.org/" target="_blank">Rogerio Feris</a>,
                  </span>
                  <span class="author-block">
                  <a href="https://mitibmwatsonailab.mit.edu/people/leonid-karlinsky/" target="_blank">Leonid Karlinsky</a>,
                  </span>
                  <span class="author-block">
                  <a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank">Trevor Darrell</a>,
                  </span>
                  <span class="author-block">
                  <a href="https://www.cs.cmu.edu/~deva/" target="_blank">Deva Ramanan</a>,
                  </span>
                  <span class="author-block">
                  <a href="https://roeiherz.github.io/" target="_blank">Roei Herzig</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Carnegie Mellon University</span><br>
                    <span class="author-block">University of California, Berkeley</span><br>
                    <span class="author-block">IBM Research</span><br>
                    <span class="author-block">MIT-IBM Watson AI Lab</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>


                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2412.00142" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link 
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/hancharikmitra/SAVs" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link 
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
           Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a wide variety of vision-language (VL) tasks such as image captioning or visual question answering. Despite strong performance, LMMs are not directly suited for foundational discriminative vision-language tasks (i.e., tasks requiring discrete label predictions) such as image classification and multiple-choice VQA. One key challenge in utilizing LMMs for discriminative tasks is the extraction of useful features from generative models. To overcome this issue, we propose an approach for finding features in the model's latent space to more effectively leverage LMMs for discriminative tasks. Toward this end, we present <b>Sparse Attention Vectors (SAVs)</b> -- a finetuning-free method that leverages sparse attention head activations (fewer than 1\% of the heads) in LMMs as strong features for VL tasks. With only few-shot examples, SAVs demonstrate state-of-the-art performance compared to a variety of few-shot and finetuned baselines on a collection of discriminative tasks. Our experiments also imply that SAVs can scale in performance with additional examples and generalize to similar tasks, establishing SAVs as both effective and robust multimodal feature representations.
          </p>

          <div class="item">
              <!-- Your image here -->
              <div style="display: flex; justify-content: center; align-items: center;">
                <img src="static/images/teaser.png" alt="Image giving an overview of Sparse Attention Vectors method.">
            </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Method Overview -->
<section class="section hero is-light2">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">What are SAVs?</h2>
        <div class="content has-text-justified">
          <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
            <b>Sparse Attention Vectors (SAVs)</b> are task-specific features extracted from the attention heads of large multimodal models (LMMs), offering a lightweight yet powerful solution for vision-language classification tasks. The method involves three primary steps:
          </p>

          <ol style="text-align: justify; font-size: 16px; line-height: 1.5; color: #333;">
            <li><b>Extracting Attention Vectors:</b> Attention vectors from all heads and layers of the transformer are computed for a given set of few-shot examples. These vectors represent the model's latent understanding of the input sequence.</li>
            <li><b>Identifying Relevant Vectors:</b> A scoring process evaluates the ability of each attention head to classify examples correctly. This step involves computing the cosine similarity between attention vectors and class centroids, selecting only the top-performing heads.</li>
            <li><b>Classification with Sparse Heads:</b> The sparse set of attention heads is then used to classify new queries. Each head votes for a class label based on its similarity to precomputed centroids, and a majority vote determines the final prediction.</li>
          </ol>

          <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
            This approach reveals a remarkable property of LMMs: even a small subset of attention heads can serve as highly effective features for downstream classification, dramatically reducing computational overhead while maintaining strong performance.
          </p>

          <div class="item">
            <!-- Your image here -->
            <div style="display: flex; justify-content: center; align-items: center;">
              <img src="static/images/detailed.png" alt="Image illustrating Sparse Attention Vectors">
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Results Section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Benchmark Results</h2>
        <div class="content has-text-justified">
          <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
            Our evaluation highlights the strength and versatility of <b>Sparse Attention Vectors (SAVs)</b> across various vision-language tasks. Below are the key insights from our results:
          </p>

          <ol style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
            <li><b>Outperformance of Zero-Shot Baselines:</b> SAVs consistently outperform leading zero-shot models like LLaVA and Qwen2-VL on tasks such as safety benchmarks, visual question answering (VQA), and image classification.</li>
            <li><b>Closing the Gap with Discriminative Models:</b> SAVs drastically reduce the performance gap with discriminative vision-language models like SigLIP and CLIP, even on tasks where these models traditionally excel.</li>
            <li><b>Superiority to Few-Shot and Finetuning Methods:</b> SAVs outperform advanced fine-tuning methods like LoRA, achieving top results on datasets such as EuroSAT and Pets, showcasing their efficiency in extracting task-relevant features without extensive resources.</li>
            <li><b>Excelling in Complex Tasks:</b> SAVs demonstrate remarkable adaptability on perception benchmarks like BLINK and NaturalBench, which require compositional reasoning and multimodal understanding, outperforming existing methods in these challenging scenarios.</li>
          </ol>

          <div class="item">
            <!-- Numerical Results Table -->
            <div style="display: flex; justify-content: center; align-items: center; margin-top: 20px;">
              <img src="static/images/results.png" alt="Numerical Results Table">
            </div>
          </div>

          <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 20px; color: #333;">
            These findings emphasize SAVs as lightweight yet powerful features for a wide array of vision-language tasks, effectively bridging the gap between general-purpose generative models and task-specific discriminative ones.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Experiments Section -->
<section class="section hero is-light2">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Analysis</h2>
        <div class="content has-text-justified">
          <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
            SAVs are effective feature representations that have a variety of desireable properties which we describe in the following:
          </p>

          <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">  
          First, we demonstrate the sparsity and interpretability of SAVs by visualizing the selected heads for hallucination detection (MHaluBench), relative depth (BLINK), and image classification (EuroSAT) tasks. We emphasize here the importance of being able to identify the exact attention heads used for specific tasks, which can be an especially useful property for usecases requiring additional explainability. Furthermore, we visualize the effectiveness of the top selected attention head in separate the task data according to class label via a t-SNE plot.
          </p>
          
          <div class="item">
            <div style="display: flex; justify-content: center; align-items: center; margin-top: 20px;">
              <img src="static/images/head_vis.png" alt="Attention Head Visualizationse">
            </div>
          </div>

          <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 20px; color: #333;">
            To evaluate the scaling properties of SAVs, we experiment with varying the number of examples per label and number of attention vectors used. We find excitingly that SAVs can scale with increasing examples and requires a sparse set of only 20 heads to achieve optimal or near optimal performance.
          </p>

          <div class="item">
            <div style="display: flex; justify-content: center; align-items: center; margin-top: 20px;">
              <img src="static/images/accuracy.png" alt="Varying number of examples and attention vectors for SAVs">
            </div>
          </div>

          <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 20px; color: #333;">
            In our paper, we perform several other experiments to better understand SAVs. The most notable of which includes the generalization of the heads that are found during SAV extraction. We find that SAV heads extracted on MHaluBench yield strong performance benefits on VLGuard and vice-versa. On the other hand, LoRA weights, as expected, do not have such generalization properties and largely overfit to the finetuned dataset. 
          </p>

          <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 20px; color: #333;">
            Another crucial benefit of SAVs is that the are truly multimodal features, able to be flexibly applied to both unimodal and multimodal tasks. This is in contrast to CLIP and SigLIP methods which have features for individual modalities. To demonstrate the benefits of our method, we compare SAVs to CLIP and SigLIP on two tasks with interleaved multimodal inputs (MHaluBench and NaturalBench). For CLIP and SigLIP we concatenate image and text features together. We find that CLIP and SigLIP vastly underperform on these tasks compared to SAVs, highlighting the value of the truly multimodal capabilities of our SAV features.
          </p>

          <div class="item">
            <div style="display: flex; justify-content: center; align-items: center; margin-top: 20px;">
              <img src="static/images/generalization_and_interleaved.png" alt="Generalization and Interleaved Tasks Experiments">
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>






<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{mitra2024sparse,
        title={Sparse Attention Vectors: Generative Multimodal Model Features Are Discriminative Vision-Language Classifiers},
        author={Mitra, Chancharik and Huang, Brandon and Chai, Tianning and Lin, Zhiqiu and Arbelle, Assaf and Feris, Rogerio and Karlinsky, Leonid and Darrell, Trevor and Ramanan, Deva and Herzig, Roei},
        journal={arXiv preprint arXiv:2412.00142},
        year={2024}
      }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
