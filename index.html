<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>[Under Construction] Sparse Attention Vectors: Generative Multimodal Model Features Are Discriminative Vision-Language Classifiers</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">[Under Construction] Sparse Attention Vectors: Generative Multimodal Model Features Are Discriminative Vision-Language Classifiers</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=k85XvocAAAAJ&hl=en" target="_blank">Chancharik Mitra</a><sup>*</sup>,
                  </span>
                  <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=dyD6nsgAAAAJ&hl=en" target="_blank">Brandon Huang</a><sup>*</sup>,
                  </span>
                  <span class="author-block">
                  <a href="https://www.linkedin.com/in/tianning-chai-64b1312a9/" target="_blank">Tianning Chai</a>,
                  </span>
                  <span class="author-block">
                  <a href="https://linzhiqiu.github.io/" target="_blank">Zhiqiu Lin</a>,
                  </span>
                  <span class="author-block">
                  <a href="https://research.ibm.com/people/assaf-arbelle" target="_blank">Assaf Arbelle</a>,
                  </span>
                  <span class="author-block">
                  <a href="https://www.rogerioferis.org/" target="_blank">Rogerio Feris</a>,
                  </span>
                  <span class="author-block">
                  <a href="https://mitibmwatsonailab.mit.edu/people/leonid-karlinsky/" target="_blank">Leonid Karlinsky</a>,
                  </span>
                  <span class="author-block">
                  <a href="https://people.eecs.berkeley.edu/~trevor/" target="_blank">Trevor Darrell</a>,
                  </span>
                  <span class="author-block">
                  <a href="https://www.cs.cmu.edu/~deva/" target="_blank">Deva Ramanan</a>,
                  </span>
                  <span class="author-block">
                  <a href="https://roeiherz.github.io/" target="_blank">Roei Herzig</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Carnegie Mellon University</span><br>
                    <span class="author-block">University of California, Berkeley</span><br>
                    <span class="author-block">IBM Research</span><br>
                    <span class="author-block">MIT-IBM Watson AI Lab</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>


                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2412.00142" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link 
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/hancharikmitra/SAVs" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link 
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
           Generative Large Multimodal Models (LMMs) like LLaVA and Qwen-VL excel at a wide variety of vision-language (VL) tasks such as image captioning or visual question answering. Despite strong performance, LMMs are not directly suited for foundational discriminative vision-language tasks (i.e., tasks requiring discrete label predictions) such as image classification and multiple-choice VQA. One key challenge in utilizing LMMs for discriminative tasks is the extraction of useful features from generative models. To overcome this issue, we propose an approach for finding features in the model's latent space to more effectively leverage LMMs for discriminative tasks. Toward this end, we present <b>Sparse Attention Vectors (SAVs)</b> -- a finetuning-free method that leverages sparse attention head activations (fewer than 1\% of the heads) in LMMs as strong features for VL tasks. With only few-shot examples, SAVs demonstrate state-of-the-art performance compared to a variety of few-shot and finetuned baselines on a collection of discriminative tasks. Our experiments also imply that SAVs can scale in performance with additional examples and generalize to similar tasks, establishing SAVs as both effective and robust multimodal feature representations.
          </p>

          <div class="item">
              <!-- Your image here -->
              <div style="display: flex; justify-content: center; align-items: center;">
                <img src="images/teaser.png" alt="Image giving an overview of Sparse Attention Vectors method.">
            </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Method Overview -->
  <section class="section hero is-light2">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Why NaturalBench?</h2>
          <div class="content has-text-justified">

            <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
              There are already many challenging visual-question-answering (VQA) benchmarks like MMMU, ScienceQA, and MMBench, so why NaturalBench? It turns out that these popular VQA benchmarks aren't as "<i>visual</i>" as they seem. For instance, many questions can be answered using <b>commonsense priors</b>, without relying on the image. For instance, ScienceQA-IMG asks "<i>What is the capital of Massachusetts?</i>", which is easily answered as "<i>Boston</i>" by a blind ChatGPT. Below, we highlight several such examples from six previous benchmarks (MME, MMBench, ScienceQA, MMMU, MMStar, AI2D) that can be solved without looking at the image:
            </p>

            <div class="item">
              <!-- Your image here -->
              <div style="display: flex; justify-content: center; align-items: center;">
                <img src="images/bias_more.jpg" alt="Image illustrating blind priors">
            </div>

              <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
                What's more, even carefully constructed benchmarks like MME and MMStar can suffer from <b>imbalanced answers</b>. For example, MME's question "<i>Does this artwork exist in the form of a painting?</i>" is answered "Yes" 97.5% of the time! We show that finetuning a "blind" GPT-3.5 — using only text and no images — on a random half of each benchmark allows it to significantly outperform random chance (see the <span style="color: red;">red dotted line</span>) on the other half. In many cases, blind GPT-3.5 even matches or surpasses LLaVA-1.5 finetuned on the same data but with images!
              </p>
              
              <div style="display: flex; justify-content: center; align-items: center;">
                <img src="images/blind_finetune.png" alt="Image illustrating finetuning results">
            </div>
            
            <p style="text-align: justify; font-size: 16px; line-height: 1.5; margin-top: 10px; color: #333;">
              These issues drive us to create NaturalBench, a vision-centric VQA benchmark immune to blind solutions.
            </p>

              
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
